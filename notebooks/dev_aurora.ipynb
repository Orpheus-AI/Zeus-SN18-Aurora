{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b4df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import cdsapi\n",
    "import torch\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from aurora import Batch, Metadata, Aurora, rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab51beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_START = pd.Timestamp(\"2025-10-13\")\n",
    "DAY_END = pd.Timestamp(\"2025-10-14\")\n",
    "\n",
    "DOWNLOAD_PATH = Path(\"../data/era5\")\n",
    "DOWNLOAD_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab0536",
   "metadata": {},
   "source": [
    "# Download all relevant ERA5 inputs for Aurora\n",
    "Can download 4GB+ of data, depending on date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbeeb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cdsapi.Client(sleep_max=10)\n",
    "\n",
    "def download_static():\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels\",\n",
    "        {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": [\n",
    "                \"geopotential\",\n",
    "                \"land_sea_mask\",\n",
    "                \"soil_type\",\n",
    "            ],\n",
    "            \"year\": \"2023\", # doesn't matter, doesn't change\n",
    "            \"month\": \"01\",\n",
    "            \"day\": \"01\",\n",
    "            \"time\": \"00:00\",\n",
    "            \"format\": \"netcdf\",\n",
    "        },\n",
    "        str(DOWNLOAD_PATH / \"static.nc\"),\n",
    "    )\n",
    "    print(\"Static variables downloaded!\")\n",
    "\n",
    "def download_data(file_name: Path, data_source, vars, pressure_levels, time=[\"00:00\", \"06:00\", \"12:00\", \"18:00\"]):\n",
    "    params = {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": vars,\n",
    "            \"year\": str(DAY_START.year),\n",
    "            \"month\": str(DAY_START.month).zfill(2),\n",
    "            \"day\": [str(date.day).zfill(2) for date in pd.date_range(DAY_START, DAY_END, freq=\"D\")],\n",
    "            \"time\": time,\n",
    "            \"format\": \"netcdf\",\n",
    "        }\n",
    "    if pressure_levels:\n",
    "        params[\"pressure_level\"] = pressure_levels\n",
    "\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    c.retrieve(\n",
    "        data_source,\n",
    "        params,\n",
    "        str(file_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4827d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 10:14:52,117 INFO Request ID is c9f81d91-ed7e-4794-b61f-1224a4ed0118\n",
      "2025-10-20 10:14:52,352 INFO status has been updated to accepted\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 1 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 2 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 3 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 4 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 5 of 500\n",
      "Retrying in 10 seconds\n",
      "2025-10-20 10:20:29,643 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7589b499967d4be2b2d077b931f961bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "afad128a23c5c9d318d3f1ea512ee6fb.nc:   0%|          | 0.00/52.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface-level variables downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 10:21:38,578 INFO Request ID is cd9b239f-9819-4a3e-88af-48537f7f1243\n",
      "2025-10-20 10:21:38,756 INFO status has been updated to accepted\n",
      "2025-10-20 10:21:44,089 INFO status has been updated to running\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 1 of 500\n",
      "Retrying in 10 seconds\n",
      "2025-10-20 10:25:52,916 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bb9da80ca747aab810f0e8436269e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3e640c84dc6739939af7a7408c9a7a4f.nc:   0%|          | 0.00/826M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atmospheric variables downloaded!\n"
     ]
    }
   ],
   "source": [
    "if not (DOWNLOAD_PATH / \"static.nc\").exists():\n",
    "   download_static()\n",
    "\n",
    "#Download the surface-level variables.\n",
    "surface_path = DOWNLOAD_PATH / \"surf_vars\" / f\"{DAY_START.strftime('%Y-%m-%d')}_{DAY_END.day}-surface-level.nc\"\n",
    "if not surface_path.exists():\n",
    "    download_data(surface_path, \"reanalysis-era5-single-levels\", [\"2m_temperature\", \"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"mean_sea_level_pressure\"], None)\n",
    "    print(\"Surface-level variables downloaded!\")\n",
    "\n",
    "# Download the atmospheric variables.\n",
    "atmos_path = DOWNLOAD_PATH / \"atmos_vars\" / f\"{DAY_START.strftime('%Y-%m-%d')}_{DAY_END.day}-atmospheric.nc\"\n",
    "if not atmos_path.exists():\n",
    "    download_data(atmos_path, \"reanalysis-era5-pressure-levels\", \n",
    "                  [\"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"specific_humidity\", \"geopotential\"], \n",
    "                  [\"50\", \"100\", \"150\", \"200\", \"250\", \"300\", \"400\", \"500\", \"600\", \"700\", \"850\", \"925\", \"1000\"])\n",
    "    print(\"Atmospheric variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a4a3e",
   "metadata": {},
   "source": [
    "# Convert data to batch for Aurora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a7d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_vars_ds = xr.open_dataset(DOWNLOAD_PATH / \"static.nc\", engine=\"netcdf4\")\n",
    "surf_vars_ds = xr.open_dataset(surface_path, engine=\"netcdf4\")\n",
    "atmos_vars_ds = xr.open_dataset(atmos_path, engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b83e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        # First select the first two time points: 00:00 and 06:00. Afterwards, `[None]`\n",
    "        # inserts a batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_vars_ds[\"t2m\"].values[:2][None]),\n",
    "        \"10u\": torch.from_numpy(surf_vars_ds[\"u10\"].values[:2][None]),\n",
    "        \"10v\": torch.from_numpy(surf_vars_ds[\"v10\"].values[:2][None]),\n",
    "        \"msl\": torch.from_numpy(surf_vars_ds[\"msl\"].values[:2][None]),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_vars_ds[\"t\"].values[:2][None]),\n",
    "        \"u\": torch.from_numpy(atmos_vars_ds[\"u\"].values[:2][None]),\n",
    "        \"v\": torch.from_numpy(atmos_vars_ds[\"v\"].values[:2][None]),\n",
    "        \"q\": torch.from_numpy(atmos_vars_ds[\"q\"].values[:2][None]),\n",
    "        \"z\": torch.from_numpy(atmos_vars_ds[\"z\"].values[:2][None]),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element. Select element 1, corresponding to time\n",
    "        # 06:00.\n",
    "        time=(surf_vars_ds.valid_time.values.astype(\"datetime64[s]\").tolist()[1],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d83c4",
   "metadata": {},
   "source": [
    "# Setup Aurora Model & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865b078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  1256300176\n"
     ]
    }
   ],
   "source": [
    "model = Aurora(use_lora=False)  # The pretrained version does not use LoRA.\n",
    "#model_path = hf_hub_download(repo_id=model.default_checkpoint_repo, filename=\"aurora-0.25-pretrained.ckpt\", cache_dir=\"/workspace/aurora/model/\")\n",
    "#model.load_checkpoint_local(model_path)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "print(\"Num parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "        steps = surf_vars_ds[\"t2m\"].shape[0] - 2\n",
    "        preds = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e835d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 13, 720, 1440])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1].atmos_vars['t'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdebf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from backend.cds_loader import CDSLoader\n",
    "\n",
    "loader = CDSLoader(cache_dir=\"../data/era5/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
