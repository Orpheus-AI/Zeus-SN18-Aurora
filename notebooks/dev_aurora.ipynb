{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b4df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import cdsapi\n",
    "import torch\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from aurora import Batch, Metadata, Aurora, rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab51beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_START = pd.Timestamp(\"2025-10-13\")\n",
    "DAY_END = pd.Timestamp(\"2025-10-14\")\n",
    "\n",
    "DOWNLOAD_PATH = Path(\"../data/era5\")\n",
    "DOWNLOAD_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab0536",
   "metadata": {},
   "source": [
    "# Download all relevant ERA5 inputs for Aurora\n",
    "Can download 4GB+ of data, depending on date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbeeb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cdsapi.Client(sleep_max=10)\n",
    "\n",
    "def download_static():\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels\",\n",
    "        {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": [\n",
    "                \"geopotential\",\n",
    "                \"land_sea_mask\",\n",
    "                \"soil_type\",\n",
    "            ],\n",
    "            \"year\": \"2023\", # doesn't matter, doesn't change\n",
    "            \"month\": \"01\",\n",
    "            \"day\": \"01\",\n",
    "            \"time\": \"00:00\",\n",
    "            \"format\": \"netcdf\",\n",
    "        },\n",
    "        str(DOWNLOAD_PATH / \"static.nc\"),\n",
    "    )\n",
    "    print(\"Static variables downloaded!\")\n",
    "\n",
    "def download_data(file_name: Path, data_source, vars, pressure_levels, time=[\"00:00\", \"06:00\", \"12:00\", \"18:00\"]):\n",
    "    params = {\n",
    "            \"product_type\": \"reanalysis\",\n",
    "            \"variable\": vars,\n",
    "            \"year\": str(DAY_START.year),\n",
    "            \"month\": str(DAY_START.month).zfill(2),\n",
    "            \"day\": [str(date.day).zfill(2) for date in pd.date_range(DAY_START, DAY_END, freq=\"D\")],\n",
    "            \"time\": time,\n",
    "            \"format\": \"netcdf\",\n",
    "        }\n",
    "    if pressure_levels:\n",
    "        params[\"pressure_level\"] = pressure_levels\n",
    "\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    c.retrieve(\n",
    "        data_source,\n",
    "        params,\n",
    "        str(file_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4827d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 10:14:52,117 INFO Request ID is c9f81d91-ed7e-4794-b61f-1224a4ed0118\n",
      "2025-10-20 10:14:52,352 INFO status has been updated to accepted\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 1 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 2 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 3 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 4 of 500\n",
      "Retrying in 10 seconds\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 5 of 500\n",
      "Retrying in 10 seconds\n",
      "2025-10-20 10:20:29,643 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7589b499967d4be2b2d077b931f961bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "afad128a23c5c9d318d3f1ea512ee6fb.nc:   0%|          | 0.00/52.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface-level variables downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 10:21:38,578 INFO Request ID is cd9b239f-9819-4a3e-88af-48537f7f1243\n",
      "2025-10-20 10:21:38,756 INFO status has been updated to accepted\n",
      "2025-10-20 10:21:44,089 INFO status has been updated to running\n",
      "Recovering from connection error [('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))], attempt 1 of 500\n",
      "Retrying in 10 seconds\n",
      "2025-10-20 10:25:52,916 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bb9da80ca747aab810f0e8436269e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3e640c84dc6739939af7a7408c9a7a4f.nc:   0%|          | 0.00/826M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atmospheric variables downloaded!\n"
     ]
    }
   ],
   "source": [
    "if not (DOWNLOAD_PATH / \"static.nc\").exists():\n",
    "   download_static()\n",
    "\n",
    "#Download the surface-level variables.\n",
    "surface_path = DOWNLOAD_PATH / \"surf_vars\" / f\"{DAY_START.strftime('%Y-%m-%d')}_{DAY_END.day}-surface-level.nc\"\n",
    "if not surface_path.exists():\n",
    "    download_data(surface_path, \"reanalysis-era5-single-levels\", [\"2m_temperature\", \"10m_u_component_of_wind\", \"10m_v_component_of_wind\", \"mean_sea_level_pressure\"], None)\n",
    "    print(\"Surface-level variables downloaded!\")\n",
    "\n",
    "# Download the atmospheric variables.\n",
    "atmos_path = DOWNLOAD_PATH / \"atmos_vars\" / f\"{DAY_START.strftime('%Y-%m-%d')}_{DAY_END.day}-atmospheric.nc\"\n",
    "if not atmos_path.exists():\n",
    "    download_data(atmos_path, \"reanalysis-era5-pressure-levels\", \n",
    "                  [\"temperature\", \"u_component_of_wind\", \"v_component_of_wind\", \"specific_humidity\", \"geopotential\"], \n",
    "                  [\"50\", \"100\", \"150\", \"200\", \"250\", \"300\", \"400\", \"500\", \"600\", \"700\", \"850\", \"925\", \"1000\"])\n",
    "    print(\"Atmospheric variables downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a4a3e",
   "metadata": {},
   "source": [
    "# Convert data to batch for Aurora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a7d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_vars_ds = xr.open_dataset(DOWNLOAD_PATH / \"static.nc\", engine=\"netcdf4\")\n",
    "surf_vars_ds = xr.open_dataset(surface_path, engine=\"netcdf4\")\n",
    "atmos_vars_ds = xr.open_dataset(atmos_path, engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        # First select the first two time points: 00:00 and 06:00. Afterwards, `[None]`\n",
    "        # inserts a batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_vars_ds[\"t2m\"].values[:2][None]),\n",
    "        \"10u\": torch.from_numpy(surf_vars_ds[\"u10\"].values[:2][None]),\n",
    "        \"10v\": torch.from_numpy(surf_vars_ds[\"v10\"].values[:2][None]),\n",
    "        \"msl\": torch.from_numpy(surf_vars_ds[\"msl\"].values[:2][None]),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_vars_ds[\"t\"].values[:2][None]),\n",
    "        \"u\": torch.from_numpy(atmos_vars_ds[\"u\"].values[:2][None]),\n",
    "        \"v\": torch.from_numpy(atmos_vars_ds[\"v\"].values[:2][None]),\n",
    "        \"q\": torch.from_numpy(atmos_vars_ds[\"q\"].values[:2][None]),\n",
    "        \"z\": torch.from_numpy(atmos_vars_ds[\"z\"].values[:2][None]),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element. Select element 1, corresponding to time\n",
    "        # 06:00.\n",
    "        time=(surf_vars_ds.valid_time.values.astype(\"datetime64[s]\").tolist()[1],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d83c4",
   "metadata": {},
   "source": [
    "# Setup Aurora Model & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865b078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  1256300176\n"
     ]
    }
   ],
   "source": [
    "model = Aurora(use_lora=False)  # The pretrained version does not use LoRA.\n",
    "#model_path = hf_hub_download(repo_id=model.default_checkpoint_repo, filename=\"aurora-0.25-pretrained.ckpt\", cache_dir=\"/workspace/aurora/model/\")\n",
    "#model.load_checkpoint_local(model_path)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "print(\"Num parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040a8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "        steps = surf_vars_ds[\"t2m\"].shape[0] - 2\n",
    "        preds = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e835d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdebf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from backend.prediction import Prediction\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "\n",
    "def hermit_interp(data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Interpolates a tensor with 6-hourly timestamps to hourly resolution\n",
    "        using Piecewise Cubic Hermite Interpolating Polynomial (PCHIP).\n",
    "\n",
    "        Args:\n",
    "            data_6hr (np.ndarray): The input data tensor with shape (x, 721, 1440).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The interpolated data tensor with shape (6(x-1), 721, 1440).\n",
    "        \"\"\"\n",
    "        # Original times: 0, 6, 12, ..., 246 hours (41 * 6)\n",
    "        t_original = np.arange(data.shape[0]) * 6\n",
    "\n",
    "        # New times: 0, 1, 2, ..., 246 hours.\n",
    "        t_new = np.arange(t_original[-1] + 1)\n",
    "\n",
    "        interpolator = PchipInterpolator(t_original, data, axis=0)\n",
    "\n",
    "        data_hourly = interpolator(t_new)\n",
    "        # remove first datapoint (only there for interpolation)\n",
    "        return data_hourly[1:]\n",
    "\n",
    "# NOTE: Works\n",
    "def _post_process_generic(data, add_first_row=True):\n",
    "        # aurora post-process\n",
    "        # Make latitude go from -87.5 to 90.0 instead of decreasing order\n",
    "        # duplicate the first row for a -90.0 prediction\n",
    "        data = data.squeeze().flip(dims=(-2,))\n",
    "        if add_first_row:\n",
    "            data = torch.cat((data[-2].unsqueeze(0), data))\n",
    "        return data\n",
    "\n",
    "def compute_surface_pressure(batch: 'Batch'):\n",
    "    # --- Physical Constants ---\n",
    "    G0 = 9.80665    # Standard gravity (m/s^2)\n",
    "    R_AIR = 287.05  # Gas constant for dry air (J/(kg*K))\n",
    "\n",
    "    # P_msl is typically in Pascals (Pa)\n",
    "    P_msl = batch.surf_vars[\"msl\"][0, -1]  \n",
    "    T_2m = batch.surf_vars[\"2t\"][0, -1]    # 2m Temperature (K)\n",
    "    z_sfc = batch.static_vars[\"z\"]         # Surface Geopotential (m^2/s^2)\n",
    "    \n",
    "    h_sfc = z_sfc / G0  # h_sfc in meters (m)\n",
    "\n",
    "    exponent = (G0 * h_sfc) / (R_AIR * T_2m)\n",
    "    \n",
    "    # Calculate P_sfc\n",
    "    P_sfc = P_msl * torch.exp(-exponent) # P_sfc in Pascals (Pa)\n",
    "    \n",
    "    return P_sfc.unsqueeze(0)\n",
    "\n",
    "\n",
    "def predict(batch: Batch, preds) -> Prediction:\n",
    "        # also include last of batch, so that interpolation for first 6 hours is possible\n",
    "        temp_preds = [_post_process_generic(batch.surf_vars[\"2t\"][:, -1], add_first_row=False)]\n",
    "        east_wind_preds = [_post_process_generic(get_100m_wind(batch, \"u\"), add_first_row=False)]\n",
    "        north_wind_preds = [_post_process_generic(get_100m_wind(batch, \"v\"), add_first_row=False)]\n",
    "\n",
    "\n",
    "        for pred in preds:\n",
    "            temp_preds.append(_post_process_generic(pred.surf_vars[\"2t\"]))\n",
    "            east_wind_preds.append(_post_process_generic(get_100m_wind(pred, \"u\")))\n",
    "            north_wind_preds.append(_post_process_generic(get_100m_wind(pred, \"v\")))     \n",
    "\n",
    "        return Prediction(\n",
    "            temperature = hermit_interp(torch.stack(temp_preds, dim=0).numpy()),\n",
    "            u_wind_100m = hermit_interp(torch.stack(east_wind_preds, dim=0).numpy()),\n",
    "            v_wind_100m = hermit_interp(torch.stack(north_wind_preds, dim=0).numpy())\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9a653648",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = xr.open_dataset(\"../data/era5/era5_t2m_2025-10-13.nc\", engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "23cdb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = _post_process_generic(torch.Tensor(gt[\"t2m\"][6].to_numpy()), add_first_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bc821c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_data = _post_process_generic(compute_surface_pressure(batch), add_first_row=False)\n",
    "#_post_process_generic(batch.surf_vars[\"10u\"][:, -1], add_first_row=False)compute_surface_pressure(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ec7a333c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(641.2620)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((gt_data - conv_data) ** 2).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cc38a20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2025, 10, 13, 6, 0),)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.metadata.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ebb45cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000.,  925.,  850.,  700.,  600.,  500.,  400.,  300.,  250.,\n",
       "        200.,  150.,  100.,   50.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atmos_vars_ds.pressure_level.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "45c2dd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.494643651601597)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.92 * np.sin(np.deg2rad(114))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5345a6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2025, 10, 13, 6, 0),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.metadata.time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ec7a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
